# -*- coding: utf-8 -*-
"""DSMLfunctions.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/11zDgj-KW_TQ1wRNf3bIE5OhE1tkmUJAT

# DSML Global Functions 01 - Data Details (info, describe, columns, etc.)
"""

# Commented out IPython magic to ensure Python compatibility.
# %capture
# Installing SHAP dependency
# When importing this file as PY library, install this before importing lib
# !pip install shap

# Use below line if cannot install simply
# !pip install git+https://github.com/openai/shap-e.git

# Install Numerize to display large numbers in readable format
# When importing this file as PY library, install this before importing lib
# !pip install numerize

# Installing SHAP from PY file for functions

import subprocess
import sys

def install(package):
  subprocess.check_call([sys.executable, "-m", "pip", "install", package])

install('shap')

import shap                         # SHapley Additive exPlanations (SHAP) for Model debugging

# Installing Numerize to make large numbers human readable

install('numerize')

from numerize import numerize     # Library to display large numbers in readable format

# Commented out IPython magic to ensure Python compatibility.
# Used to ignore the warning given as output of the code
import warnings
warnings.filterwarnings('ignore')

# import the important packages
import numpy as np                    # NumPy is used for arrays
import pandas as pd                   # Pandas is used for data manipulation, especially through DataFrames
import re                             # Regulat expressions for date format
from datetime import datetime, date   # Date string manipulation
from IPython.display import display   # Display dataframes nicely
import matplotlib.pyplot as plt       # Matplotlib's PyPlot is used for plots and visualizations
import seaborn as sns                 # SeaBorn is for advanced graphs or better style
sns.set_style('darkgrid')

# Magic Function that displays results inline after code block
# %matplotlib inline

from sklearn.cluster import KMeans                  # KMeans clustering
from sklearn.linear_model import LinearRegression   # Linear Regression Model

# Multicollinearity using VIF
from statsmodels.stats.outliers_influence import variance_inflation_factor

# Get Metrics to evlauate models: MAE, MAPE, MSE, R2
from sklearn.metrics import mean_absolute_error, mean_absolute_percentage_error, mean_squared_error, r2_score

import scipy.stats as stats       # SciPy Stats contains probability distributions and statistical functions
import math

# Importing packages that need to be installed to Google Colab
# Comment out imports for importing notebook as PY file

# Progress bar for loops
#for item in progress_bar(my_list):
#    process(item)
from fastprogress import master_bar, progress_bar

# Import the TEST dataset
# data = pd.read_csv('/content/drive/MyDrive/DSML/GL-Capstone-Houses-03-2023/Dataset+-+House+Price+Prediction+-+innercity.xlsx+-+innercity.csv')

def getGeneralInfo(data):
  print('')
  print('===============')
  print('Data Head')
  print('===============')
  display(data.head(5))

  print('')
  print('===============')
  print('Data Shape')
  print('===============')
  print(data.shape)

  print('')
  print('===============')
  print('Data Info')
  print('===============')
  print(data.info())

  print('')
  print('===============')
  print('Number Columns')
  print('===============')
  num_cols = data.select_dtypes(include=["number"])
  display(num_cols.head(5))
  print(num_cols.shape)

  print('')
  print('===============')
  print('Object Columns')
  print('===============')
  obj_cols = data.select_dtypes(exclude=["number"])
  display(obj_cols.head(5))
  print(obj_cols.shape)

  print('')
  print('===============')
  print('Unique Values in all Columns')
  over70 = []
  under10 = []
  print('===============')
  for col in data.columns:
    print(data[col].value_counts())
    print("Total unique values in",col,":", data[col].nunique(), "-", '{:.2%}'.format(data[col].nunique()/len(data[col])), "of", len(data[col]))
    if (data[col].nunique()/len(data[col]) >= 0.7):
      over70.append(col)
    if (data[col].nunique()/len(data[col]) <= 0.1):
      under10.append(col)
    print('-------------')
  print('-------------')
  print("Columns with >= 70% unique values:", len(over70))
  if len(over70) > 0:
    print(over70)
  print("")
  print("Columns with <= 10% unique values:", len(under10))
  if len(under10) > 0:
    print(under10)

def getDataDescribe(data):
  print('')
  print('===============')
  print('Data Describe')
  print('===============')
  display(data.describe().T.apply(lambda s: s.apply('{0:.5f}'.format)))

def getMissingValues(data):
  m = pd.DataFrame({'Count': data.isnull().sum()})
  m['Percent'] = pd.Series([(val / len(data)) for val in m['Count']], index = m.index)
  m['Percent %'] = pd.Series(["{0:.2%}".format(val / len(data)) for val in m['Count']], index = m.index)

  n = m[m['Percent'] >= .80]

  m.drop('Percent', axis=1, inplace=True)
  n.drop('Percent', axis=1, inplace=True)

  print('')
  print('===============')
  print('Missing Values in all Columns - Total Columns:', len(m))
  print('===============')
  display(m[m['Count'] >= 0].sort_values(['Count'], ascending=[False]))

  print('')
  print('===============')
  print('Missing Values >= 80% - Total Columns:', len(n))
  print('===============')
  display(n.sort_values(['Count'], ascending=[False]))

"""# DSML Global Functions 02 - Null Handling"""

def getNullHandler(data):
  print('')
  print('===============')
  print('Data Null Handler')
  print('===============')
  print('Nulls present:',data.isnull().values.any())
  print('Nulls count:',data.isnull().sum().sum())
  print(data.isnull().sum())
  print('---------------')
  print('Imputing Object columns with most frequent (mode) and Number columns with middle value (median)')
  # Imputing missing values with mode (most frequent) for Object columns and with median for all other columns
  for col in data.columns:
      if data[col].dtype == "object":
          data[col].fillna(value=data[col].mode()[0], inplace=True)
      elif data[col].dtype == "int64" or data[col].dtype =="float64":
          data[col].fillna(value=data[col].median(), inplace=True)
      # return data
  print('---------------')
  print('Confirming no nulls left')
  print('Nulls present:',data.isnull().values.any())
  print('Nulls count:',data.isnull().sum().sum())
  return data

"""# DSML Global Functions 03 - Data Cleanup

## Date Formats
"""

def getDateHandler(data, date_column, current_format):
  dates = data[date_column].apply(lambda d: datetime.strptime(d, current_format))
  # Create dummy columns
  data['year'] = [d.year for d in dates]
  data['month'] = [d.month for d in dates]
  data['day'] = [d.day for d in dates]
  # drop the date column
  data = data.drop(columns=[date_column])

  display(data.head(5))
  return data, dates

"""## Remove Rows Where Column Has Specific Value"""

def getRemoveRowsWithValue(data, columnList, value):
  print('')
  print('===============')
  print(f'Removing "{value}" from columns')
  print('===============')
  for column in columnList:
    print('')
    print(f'--- Removing "{value}" from "{column}" ---')
    data.drop(data[data[column] == value].index, inplace = True)
    print(data[column].value_counts())
  return data

"""### Convert Object Column to Number"""

def getConvertObjectToNumber(data, columnList):
  print('')
  print('===============')
  print(f'Converting Columns to Number')
  print('===============')
  for column in columnList:
    print('')
    data[column] = pd.to_numeric(data[column])
    print(f'--- Converted "{column}" to {data[column].dtypes} ---')
  return data

"""### Handling Outliers"""

def getOutliersHandler(data, excluded_columns, factor = 1.5):
  num_cols = data.select_dtypes(include=["number"])
  data_outliers_fixed = data.copy()

  k = len(num_cols.columns)
  n = 4
  m = (k - 1) // n + 1
  print('')
  print('===============')
  print(f'Removing Outliers with IQR factor of: {factor}')
  print('===============')
  for col in num_cols:
    if col in excluded_columns:
      continue
    else:
      # r, c = i // n, i % n
      q1 = data_outliers_fixed[col].quantile(.25)
      q3 = data_outliers_fixed[col].quantile(.75)
      iqr = q3 - q1
      upper_whisker = q3 + (factor * iqr)
      lower_whisker = q1 - (factor * iqr)
      data_outliers_fixed[col] = np.where(data_outliers_fixed[col] > upper_whisker, upper_whisker, np.where(data_outliers_fixed[col] < lower_whisker, lower_whisker, data_outliers_fixed[col]))

      plt.figure(figsize=(n * 5, m))
      plt.subplot(141)
      sns.histplot(data[col],  stat='density', label="skew: " + str(np.round(data[col].skew(), 2)))
      sns.kdeplot(data[col], color='red', warn_singular=False)
      plt.title('Before', fontsize=20)
      plt.legend()

      plt.subplot(142)
      ax = sns.histplot(data_outliers_fixed[col],  stat='density', label="skew: " + str(np.round(data_outliers_fixed[col].skew(), 2)))
      sns.kdeplot(data_outliers_fixed[col], color='red', warn_singular=False)
      plt.title('After', fontsize=20)
      plt.legend()

      plt.subplot(143)
      sns.boxplot(x = data[col])
      plt.title('Before', fontsize=20)

      plt.subplot(144)
      sns.boxplot(x = data_outliers_fixed[col])
      plt.title('After', fontsize=20)

      plt.tight_layout()
      plt.show()
  return data_outliers_fixed

"""# DSML Global Functions 04 - Data Graphs (Histograms & Heat Map)

## Univariate Graphs
"""

def getGeneralHistograms(data):
  num_cols = data.select_dtypes(include=["number"])
  k = len(num_cols.columns)
  n = 4
  m = (k - 1) // n + 1
  fig, axes = plt.subplots(m, n, figsize=(n * 5, m * 3))
  print('')
  print('===============')
  print('Data Numerical Histograms')
  print('===============')
  for i, (name, col) in enumerate(num_cols.iteritems()):
      r, c = i // n, i % n
      ax = axes[r, c]
      col.hist(ax=ax)
      ax2 = col.plot.kde(ax=ax, secondary_y=True, title=name, color='red')
      ax2.set_ylim(0)
  fig.tight_layout()
  # plt.title("Histograms", fontsize=20)
  plt.show()

def getGeneralBoxPlots(data):
  num_cols = data.select_dtypes(include=["number"])
  k = len(num_cols.columns)
  n = 4
  m = (k - 1) // n + 1
  fig, axes = plt.subplots(m, n, figsize=(n * 5, m * 3))
  print('')
  print('===============')
  print('Data Numerical Box Plots')
  print('===============')
  for i, col in enumerate(num_cols):
    r, c = i // n, i % n
    sns.boxplot(x = data[col], ax=axes[r, c])
  fig.tight_layout()
  # plt.title("Box Plots", fontsize=20)
  plt.show()

def getGeneralBoxPlotSkew(data):
  num_cols = data.select_dtypes(include=["number"])
  k = len(num_cols.columns)
  n = 4
  m = (k - 1) // n + 1
  print('')
  print('===============')
  print('Box Plots and Skew')
  print('===============')
  for i, col in enumerate(num_cols):
    fig = plt.figure(figsize=(n * 5, m))
    plt.subplot(131)
    ax = sns.histplot(data[col], kde=True, label="skew: " + str(np.round(data[col].skew(), 2)))
    ax.lines[0].set_color('red')
    plt.legend()
    plt.subplot(132)
    sns.boxplot(x = data[col])
    plt.subplot(133)
    stats.probplot(data[col], plot=plt)
    fig.tight_layout()
    # plt.title("Box Plot Skewness", fontsize=20)
    plt.show()

"""## Bivariate Graphs"""

def getGeneralPairplot(data, target_column):
  sns.pairplot(data, hue=target_column)
  plt.title(f"Pair Plot vs {target_column}", fontsize=20)
  plt.show()

def getGeneralGraphRegression(data, target_column):
  num_cols = data.select_dtypes(include=["number"])
  k = len(num_cols.columns)
  n = 4
  m = (k - 1) // n + 1
  fig = plt.subplots(figsize=(n * 5, m * 3))
  print('')
  print('===============')
  print(f'Data Numerical Scatter Plot with Regression for column: {target_column}')
  print('===============')
  Y = num_cols[target_column].values.reshape(-1, 1)  # df.iloc[:, 4] is the column of Y
  for i, (name, col) in enumerate(num_cols.iteritems()):
    r, c = i // n, i % n
    X = num_cols.iloc[:, i].values.reshape(-1, 1)  # iloc[:, 1] is the column of X
    linear_regressor = LinearRegression()
    linear_regressor.fit(X, Y)
    Y_pred = linear_regressor.predict(X)

    plt.subplot(m, n, i+1)
    plt.scatter(X, Y)
    plt.plot(X, Y_pred, color='red')
    plt.title(name)
  plt.show()

def getGeneralHeatMap(data, show_values = True):
  print('')
  print('===============')
  print('Data Heat Map')
  print('===============')
  num_cols = data.select_dtypes(include=["number"])
  corr=data.corr()

  # Mask to remove upper triangle
  mask = np.triu(np.ones_like(corr, dtype=bool))

  fig, ax = plt.subplots(figsize=(20,20))
  # cmap = sns.diverging_palette(230, 20, as_cmap = True) # red colors

  sns.heatmap(corr, annot = show_values, fmt = '.2f', cmap = 'mako', linewidths=1, mask=mask, vmax=.3, center=0, square=True, cbar_kws={"shrink": .5})
  plt.title("Heat Map", fontsize=20)
  plt.show()

"""# DSML Global Functions 05 - Feature Engineering"""

def createSeasons(data, month_col):
  data["sell_season"] = np.where(
        (data[month_col] == 12) | (data[month_col] == 1) | (data[month_col] == 2), 'winter', np.where(
          (data[month_col] == 3) | (data[month_col] == 4) | (data[month_col] == 5), 'spring', np.where(
            (data[month_col] == 6) | (data[month_col] == 7) | (data[month_col] == 8), 'summer', np.where(
              (data[month_col] == 9) | (data[month_col] == 10) | (data[month_col] == 11), 'autumn', ''
              )
            )
          )
        )
  season_cols = [month_col, 'sell_season']
  display(data[season_cols].head())
  return data

def createFeatureAge(data, feature_name, feature_year):
  today = date.today()
  year = today.year
  print(f'Current year: {year}')

  data[f'{feature_name}_age'] = np.where(
      ((year - data[feature_year]) < 10), 'under_10y', np.where(
          ((year - data[feature_year]) >= 10) & ((year - data[feature_year]) < 30), 'between_10y_30y', np.where(
              ((year - data[feature_year]) >= 30) & ((year - data[feature_year]) < 50), 'between_30y_50y', np.where(
                  ((year - data[feature_year]) >= 50), 'over_50y', 'over_50y'
              )
          )
      )
  )
  age_cols = [f'{feature_name}_age', feature_year]
  display(data[age_cols].head())
  return data

def compareTarget(data, target, compare):
  fig = plt.figure(figsize=(30, 10))

  plt.subplot(121)
  data[compare].value_counts().sort_values().plot.bar()
  plt.title(compare, fontsize=20)

  plt.subplot(122)
  plt.scatter(data[compare].sort_values(), data[target])
  plt.title(f'{compare} vs {target}', fontsize=20)

  plt.show()

def makeHotEncodeDummyCols(data, columnList):
  all_dummy_cols = pd.DataFrame()
  print('')
  print('===============')
  print('Making Hot Encoded Dummy Columns')
  print('===============')
  for col in columnList:
    print('')
    print(f'Dummy column suffixes for column {col}_x:')
    unique = data[col].unique()
    print(unique)
    for value in unique:
      unique_col_name = f'{col}_{value}'
      all_dummy_cols[unique_col_name] = np.where(data[col] == value, 1, 0)
    print('---------------')
  print('')
  print('===============')
  print('Table Head of Hot Encoded Dummy Columns')
  print('===============')
  return all_dummy_cols

def makeHotYesNoCols(data, yes_no_cols):
  all_dummy_cols = pd.DataFrame()
  print('')
  print('===============')
  print('Making Hot Encoded Yes/No Columns')
  print('===============')
  for col in yes_no_cols:
    print('')
    print(f'Hot encoding Yes/No from {col}:')
    all_dummy_cols[col] = np.where(data[col] == 'Yes', 1, np.where(data[col] == 'YES', 1, np.where(data[col] == 'Y', 1, np.where(data[col] == 'y', 1, 0))))
  unique = data[col].value_counts()
  unique_dummy = all_dummy_cols.value_counts()
  display(unique, unique_dummy)
  return all_dummy_cols

"""# DSML Global Functions 06 - Clustering

## KMeans (number columns only)
"""

# Output number of clusters

def getKMeansCurve(data, max_k):
  print('')
  print('===============')
  print('K-Means Curve')
  print('===============')
  num_cols = data.select_dtypes(include=["number"])
  means = []
  inertias = []

  for k in range(1, max_k):
    kmeans = KMeans(n_clusters=k, n_init=1)
    kmeans.fit(num_cols)

    means.append(k)
    inertias.append(kmeans.inertia_)

  # Generate the elbow plot
  fig = plt.subplots(figsize=(10,5))
  plt.plot(means, inertias, 'o-')
  plt.xlabel('Number of clusters')
  plt.ylabel('Inertia')
  plt.grid(True)
  plt.title(f"KMeans Elbow Curve", fontsize=20)
  plt.show()

# Apply KMeans

def getKMeansLabels(data, k):
  print('')
  print('===============')
  print('K-Means Labels')
  print('===============')
  num_cols = data.select_dtypes(include=["number"])

  kmeans = KMeans(n_clusters=k, n_init=1)
  kmeans.fit(num_cols)

  data["k_means_labels"] = kmeans.labels_

  return data

# Plot KMeans in Scatter Plot of X, Y

def getGroupedScatterPlot(data, x_col, y_col, labels_col):
  print('')
  print('===============')
  print('Grouped Scatter Plot')
  print('===============')
  num_cols = data.select_dtypes(include=["number"])

  sns.lmplot(x=x_col, y=y_col, data=num_cols, hue=labels_col, fit_reg=False, legend=False, height=15, aspect=1)

  # Move the legend to an empty part of the plot
  plt.legend(loc='lower right')
  plt.title(f"{x_col} vs {y_col} grouped by: {labels_col}", fontsize=20)
  plt.show()

"""# DSML Global Functions 07 - Multicollinearity"""

#Use this on X_train data with a default correlation threshold of abs(0.7)

def getMulticollHandler(data, threshold = 0.7):
  print('')
  print('===============')
  print(f'Multicollinearity Threshold: {threshold}')
  print('===============')
  correlated_cols = set() # get unique elements only
  corr_matrix = data.corr()
  for i in range(len(corr_matrix.columns)):
    for j in range(i):
      if abs(corr_matrix.iloc[i,j]) > threshold:
        colname = corr_matrix.columns[i]
        correlated_cols.add(colname)

  display(correlated_cols)

  return correlated_cols

"""## View VIF of X_train or X_test

General Rule of thumb: If VIF is 1 then there is no correlation between the kth predictor and the remaining predictor variables, and  hence the variance of β̂k is not inflated at all. Whereas if **VIF exceeds 5 or is close to exceeding 5, we say there is moderate VIF and if it is 10 or exceeding 10, it shows signs of high multi-collinearity.**
"""

# Function to check VIF
def getVIF(data):
  print('')
  print('===============')
  print('VIF')
  print('===============')
  vif = pd.DataFrame()
  vif["feature"] = data.columns

  # Calculating VIF for each feature
  vif["VIF"] = [
      variance_inflation_factor(data.values, i) for i in range(len(data.columns))
  ]
  # print(vif.loc[vif['VIF'].isnull()], 'feature')
  # print(vif.loc[vif['VIF'] > 10], 'feature')

  print(vif)
  return vif

"""# TO DO - DSML Global Functions 08 - Cross Validation Comparison
Compare previous models to find cross validation
"""

# # comparing the x_trains: X_train_scaled, X_train_multicoll, X_train_ols, X_train_ols_pvalue, X_train_ols_vif

# # build the regression model using Sklearn Linear regression
# linearregression = LinearRegression()

# x_train_variables = {
#     'X_train_scaled': X_train_scaled,
#     'X_train_multicoll': X_train_multicoll,
#     'X_train_ols': X_train_ols,
#     'X_train_ols_pvalue': X_train_ols_pvalue,
#     'X_train_ols_vif': X_train_ols_vif
# }

# for key, value in x_train_variables.items():
#   index = list(x_train_variables.keys()).index(key)
#   print(f"Model {index}: {key}")

#   # cv=10 represents data is divided into 10 folds
#   cv_Score11_train = cross_val_score(linearregression, value, y_train, cv = 10)
#   cv_Score12_train = cross_val_score(linearregression, value, y_train, cv = 10, scoring = 'neg_mean_squared_error')

#   print("RSquared: %0.3f (+/- %0.3f)" % (cv_Score11_train.mean(), cv_Score11_train.std() * 2))
#   print("Mean Squared Error: %0.3f (+/- %0.3f)" % (-1*cv_Score12_train.mean(), cv_Score12_train.std() * 2))
#   print('-------------------------------------')
#   print('')

"""# DSML Global Functions 09 - Boost Regression Feature Importance Charts
- For XGBoost, ADABoost, CatBoost
- Depends on SHAP lib for model check
"""

# Function to chart most important features

def getFeatureImportanceCharts(model, X_train, X_test):
  print('')
  print('===============')
  print('Features Sorted by Importance')
  print('===============')
  sorted_idx = model.feature_importances_.argsort()
  plt.barh(X_train.columns[sorted_idx],
          model.feature_importances_[sorted_idx],
          # color='turquoise'
          )
  plt.xlabel("Feature Importance")
  plt.show()

# Get pie chart of most important features
  print('')
  print('===============')
  print('Features Chart')
  print('===============')
  data = model.feature_importances_[sorted_idx]
  labels = X_train.columns[sorted_idx]
  plt.pie(data, autopct='%1.1f%%', pctdistance=.80, labels=labels)

  # draw circle
  centre_circle = plt.Circle((0, 0), 0.60, fc='white')
  fig = plt.gcf()
  # Adding Circle in Pie chart
  fig.gca().add_artist(centre_circle)
  plt.xlabel("Feature Importance")
  # Add Legends
  # plt.legend(labels, loc="upper right")
  plt.show()

  # SHAP Explainer of feature importance
# the features are ranked based on their average absolute SHAP. The higher the SHAP value, the larger the predictor’s attribution.

  print('')
  print('===============')
  print('Features SHAP value')
  print('===============')
  explainer = shap.TreeExplainer(model)
  shap_values = explainer.shap_values(X_test)
  shap.summary_plot(shap_values, X_test, feature_names = X_train.columns[sorted_idx])

"""# DSML Global Functions XX - Model Comparison Table
## SciKit Metrics MAE, MAPE, RMSE, RSQUARE
"""

# Receive a Comparison Table to add the calculated metrics
def getModelComparisonMetrics(model_comparison_table, model_number, model_title, y_train, y_test, y_pred_train, y_pred_test):
  print('')
  print('===============')
  print(f'Model Comparison v{model_number}')
  print('===============')

  # Getting MAE
  mae_train = numerize.numerize(mean_absolute_error(y_train, y_pred_train), 2)
  mae_test = numerize.numerize(mean_absolute_error(y_test, y_pred_test), 2)

  # Getting MAPE
  mape_train = f'{np.round(mean_absolute_percentage_error(y_train, y_pred_train) * 100, 2)}%'
  mape_test = f'{np.round(mean_absolute_percentage_error(y_test, y_pred_test) * 100, 2)}%'

  # Getting RMSE
  RMSE_train = numerize.numerize(mean_squared_error(y_train, y_pred_train), 2)
  RMSE_test = numerize.numerize(mean_squared_error(y_test, y_pred_test), 2)

  # Getting R-Square
  RSquare_train = f'{np.round(r2_score(y_train, y_pred_train) * 100, 2)}%'
  RSquare_test = f'{np.round(r2_score(y_test, y_pred_test) * 100, 2)}%'

  base_results_train = pd.DataFrame({'MAE': mae_train, 'MAPE': mape_train, 'RMSE':RMSE_train, 'RSquare': RSquare_train}, index=[f'Model {model_number} Train: {model_title}'])
  base_results_test = pd.DataFrame({'MAE': mae_test, 'MAPE': mape_test, 'RMSE':RMSE_test, 'RSquare': RSquare_test}, index=[f'Model {model_number} Test: {model_title}'])

  model_comparison_table = model_comparison_table.append(base_results_train)
  model_comparison_table = model_comparison_table.append(base_results_test)

  display(model_comparison_table)

  return model_comparison_table

"""# Testing the functions"""

# Testing the Functions

# --- INFO
# getGeneralInfo(data)
# getDataDescribe(data)

# --- CLEANUP
# getDateHandler(data, 'dayhours', '%Y%m%dT000000')

# data = getNullHandler(data)

# columnList = ['ceil', 'coast', 'condition', 'yr_built', 'long', 'total_area']   # List of columns that are of Object type
# data = getRemoveRowsWithValue(data, columnList, '$')
# data = getConvertObjectToNumber(data, columnList)

# --- UNIVARIATE GRAPHS
# getGeneralHistograms(data)
# getGeneralBoxPlots(data)

# --- BIVARIATE GRAPHS
# getGeneralPairplot(data, 'price')
# getGeneralGraphRegression(data, 'price')  # Regression compares against TARGET column
# getGeneralHeatMap(data, True)

# getGeneralBoxPlotSkew(data)

# --- OUTLIERS


# Removing columns that are of Number type but are either 1/0, categorical, date or location
# excluded_columns = ['furnished', 'coast', 'sight', 'condition', 'quality', 'yr_built', 'yr_renovated', 'lat', 'long', 'zipcode']
# data = getOutliersHandler(data, excluded_columns)
# data.describe().T

# --- CLUSTERING

# getKMeansCurve(data, 10)

# data = getKMeansLabels(data, 3)
# data.head()

# getGroupedScatterPlot(data, 'long', 'lat', 'k_means_labels')

# --- Multicollinearity
# Need to remove nulls before

# correlated_columns = getMulticollHandler(data, .7)

